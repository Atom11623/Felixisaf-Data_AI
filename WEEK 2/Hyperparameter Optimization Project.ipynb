{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d42724f0-22fd-4c00-943a-9702ca7844f0",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization Project\r\n",
    "\r\n",
    "## Overview\r\n",
    "This project focuses on hyperparameter optimization to improve the performance of machine learning models. Using a diabetes prediction dataset, we explore Random Forest and XGBoost classifiers to systematically determine the best hyperparameters for optimal model performance. Additionally, we leverage Explainable AI (XAI) tools such as LIME and SHAP to interpret model predictions, ensuring transparency and trust in the results.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Dataset Description\r\n",
    "\r\n",
    "### Source\r\n",
    "The dataset originates from the National Institute of Diabetes and Digestive and Kidney Diseases. It aims to predict whether a patient has diabetes based on several diagnostic metrics.\r\n",
    "\r\n",
    "### Characteristics\r\n",
    "- **Number of Instances**: 768\r\n",
    "- **Features**: 8 independent variables and 1 target variable\r\n",
    "- **Target Variable**: `Outcome` (1 for diabetic, 0 for non-diabetic)\r\n",
    "- **Key Features**:\r\n",
    "  - Glucose\r\n",
    "  - Blood Pressure\r\n",
    "  - BMI (Body Mass Index)\r\n",
    "  - Age\r\n",
    "  - Diabetes Pedigree Function\r\n",
    "- **Data Quality**: The dataset is clean with no missing values.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Workflow\r\n",
    "\r\n",
    "### 1. Data Preprocessing\r\n",
    "- The data is split into training and testing sets using an 80-20 split.\r\n",
    "- Independent variables (`X`) and the target variable (`y`) are separated for analysis.\r\n",
    "\r\n",
    "### 2. Hyperparameter Optimization\r\n",
    "\r\n",
    "#### Random Forest\r\n",
    "- **Parameters Tuned**:\r\n",
    "  - Number of estimators (`n_estimators`): [100, 200, 300]\r\n",
    "  - Maximum depth of trees (`max_depth`): [None, 10, 20]\r\n",
    "  - Minimum samples required to split a node (`min_samples_split`): [2, 5, 10]\r\n",
    "- **Method**: Grid Search with Cross-Validation (CV).\r\n",
    "- **Optimal Parameters**:\r\n",
    "  - `n_estimators`: 200\r\n",
    "  - `max_depth`: None\r\n",
    "  - `min_samples_split`: 10\r\n",
    "\r\n",
    "#### XGBoost\r\n",
    "- **Parameters Tuned**:\r\n",
    "  - Number of estimators (`n_estimators`): [100, 200, 300]\r\n",
    "  - Maximum depth of trees (`max_depth`): [3, 6, 9]\r\n",
    "  - Learning rate (`learning_rate`): [0.01, 0.1, 0.2]\r\n",
    "- **Method**: Grid Search with Cross-Validation (CV).\r\n",
    "- **Optimal Parameters**:\r\n",
    "  - `n_estimators`: 200\r\n",
    "  - `max_depth`: 3\r\n",
    "  - `learning_rate`: 0.01\r\n",
    "\r\n",
    "### 3. Model Evaluation\r\n",
    "Performance metrics were calculated for both models:\r\n",
    "\r\n",
    "| Metric          | Random Forest | XGBoost   |\r\n",
    "|------------------|---------------|-----------|\r\n",
    "| Accuracy         | 0.7489        | 0.7576    |\r\n",
    "| Precision        | 0.6310        | 0.6667    |\r\n",
    "| Recall           | 0.6625        | 0.6000    |\r\n",
    "| F1 Score         | 0.6463        | 0.6316    |\r\n",
    "| ROC-AUC          | 0.8148        | 0.8063    |\r\n",
    "\r\n",
    "**Observations**:\r\n",
    "- **Random Forest**: Higher recall and ROC-AUC, making it suitable for identifying positive cases.\r\n",
    "- **XGBoost**: Better accuracy and precision, reducing false positives.\r\n",
    "\r\n",
    "### 4. Explainable AI\r\n",
    "#### LIME (Local Interpretable Model-Agnostic Explanations)\r\n",
    "- **Findings**:\r\n",
    "  - LIME was used to explain individual predictions by analyzing the contribution of each feature to the model's output.\r\n",
    "  - For a sample prediction, features like `Glucose`, `BMI`, and `Age` had the highest impact.\r\n",
    "    - **Example**: A prediction with high glucose levels (98 mg/dL) and elevated BMI (34 kg/mÂ²) was classified as diabetic due to their strong influence on the probability.\r\n",
    "    - LIME highlighted that features like `SkinThickness` and `BloodPressure` had less influence on certain predictions.\r\n",
    "- **Explanation**:\r\n",
    "  - LIME provides an intuitive breakdown of how features individually contribute to the outcome, making it easier to trust and validate the model in sensitive applications like healthcare.\r\n",
    "\r\n",
    "#### SHAP (SHapley Additive exPlanations)\r\n",
    "- **Findings**:\r\n",
    "  - SHAP values revealed global feature importance, showing that `Glucose` was consistently the most significant predictor for diabetes.\r\n",
    "  - The summary plot indicated:\r\n",
    "    - High glucose levels increased the likelihood of being classified as diabetic.\r\n",
    "    - Features like `BMI` and `Age` had moderate but consistent contributions across predictions.\r\n",
    "    - Features like `DiabetesPedigreeFunction` provided subtle yet valuable signals for certain predictions.\r\n",
    "  - SHAP dependence plots illustrated the non-linear relationships between features and the target variable.\r\n",
    "- **Explanation**:\r\n",
    "  - SHAP enhances global interpretability by showing how much each feature contributes to the model's predictions on average.\r\n",
    "  - It provides actionable insights for stakeholders, such as identifying high-risk individuals based on glucose and BMI thresholds.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Tools and Libraries\r\n",
    "\r\n",
    "### Python Libraries\r\n",
    "- **Data Handling**: `pandas`, `numpy`\r\n",
    "- **Modeling**: `scikit-learn`, `xgboost`\r\n",
    "- **Visualization**: `matplotlib`\r\n",
    "- **Explainability**: `lime`, `shap`\r\n",
    "\r\n",
    "### Environment\r\n",
    "- Development was conducted using Jupyter Notebooks and Google Colab.\r\n",
    "- Python 3.9 was used for compatibility.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## How to Use\r\n",
    "\r\n",
    "1. **Install Dependencies**:\r\n",
    "   Run the following command to install required libraries:\r\n",
    "   ```bash\r\n",
    "   pip install scikit-learn xgboost shap lime pandas numpy matplotlib\r\n",
    "   ```\r\n",
    "\r\n",
    "2. **Prepare the Dataset**:\r\n",
    "   - Place the dataset (`diabetes.csv`) in the project directory.\r\n",
    "\r\n",
    "3. **Run the Script**:\r\n",
    "   - Execute the notebook or Python script to:\r\n",
    "     - Perform hyperparameter tuning.\r\n",
    "     - Train and evaluate models.\r\n",
    "     - Generate explainability insights.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Results and Insights\r\n",
    "- **Best Model**: XGBoost achieved slightly higher accuracy.\r\n",
    "- **Feature Importance**: Glucose, BMI, and Age are the most influential predictors.\r\n",
    "- **Use Cases**:\r\n",
    "  - **Random Forest**: Suitable for scenarios where recall (minimizing false negatives) is critical, such as medical diagnosis.\r\n",
    "  - **XGBoost**: Ideal for cases prioritizing precision to avoid false positives.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Future Enhancements\r\n",
    "- Integrate additional models like Support Vector Machines (SVM) or Neural Networks.\r\n",
    "- Automate the hyperparameter tuning process using Bayesian Optimization.\r\n",
    "- Build an interactive dashboard to visualize model performance and predictions.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Contact\r\n",
    "For further inquiries, please reach out to the project contributor.\r\n",
    "\r\n",
    "---\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3add1fbb-5036-4d38-b945-795c8bff99e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
